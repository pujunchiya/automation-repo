{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-30T10:21:57.352406Z","iopub.execute_input":"2023-09-30T10:21:57.352841Z","iopub.status.idle":"2023-09-30T10:21:57.858846Z","shell.execute_reply.started":"2023-09-30T10:21:57.352805Z","shell.execute_reply":"2023-09-30T10:21:57.858067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loading","metadata":{}},{"cell_type":"code","source":"\n# Load the datasets\nsummaries_train_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\nsummaries_test_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\nprompts_train_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\nprompts_test_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\n\n# Display the shape of each dataset\ndata_shapes = {\n    \"summaries_train_df\": summaries_train_df.shape,\n    \"summaries_test_df\": summaries_test_df.shape,\n    \"prompts_train_df\": prompts_train_df.shape,\n    \"prompts_test_df\": prompts_test_df.shape\n}\n\ndata_shapes\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:57.860479Z","iopub.execute_input":"2023-09-30T10:21:57.861093Z","iopub.status.idle":"2023-09-30T10:21:57.988587Z","shell.execute_reply.started":"2023-09-30T10:21:57.861063Z","shell.execute_reply":"2023-09-30T10:21:57.987910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values in each dataset\nmissing_values = {\n    \"summaries_train_df\": summaries_train_df.isnull().sum(),\n    \"summaries_test_df\": summaries_test_df.isnull().sum(),\n    \"prompts_train_df\": prompts_train_df.isnull().sum(),\n    \"prompts_test_df\": prompts_test_df.isnull().sum()\n}\n\nmissing_values\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:57.989918Z","iopub.execute_input":"2023-09-30T10:21:57.990431Z","iopub.status.idle":"2023-09-30T10:21:58.003880Z","shell.execute_reply.started":"2023-09-30T10:21:57.990403Z","shell.execute_reply":"2023-09-30T10:21:58.002742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Summaries Statistics:\n\nCheck for missing values.\nDistribution of 'content' and 'wording' scores.\nAverage summary length.\nPrompts Statistics:\n\nNumber of unique prompts.\nAverage length of prompt questions and texts.","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Summaries Statistics\n\n# Checking for missing values\nmissing_values = summaries_train_df.isnull().sum()\n\n# Distribution of 'content' and 'wording' scores\ncontent_desc = summaries_train_df['content'].describe()\nwording_desc = summaries_train_df['wording'].describe()\n\n# Average summary length\nsummaries_train_df['summary_length'] = summaries_train_df['text'].apply(len)\navg_summary_length = summaries_train_df['summary_length'].mean()\n\nmissing_values, content_desc, wording_desc, avg_summary_length\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:58.006236Z","iopub.execute_input":"2023-09-30T10:21:58.006642Z","iopub.status.idle":"2023-09-30T10:21:58.039863Z","shell.execute_reply.started":"2023-09-30T10:21:58.006596Z","shell.execute_reply":"2023-09-30T10:21:58.038845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prompts Statistics\n\n# Number of unique prompts\nnum_unique_prompts = prompts_train_df['prompt_id'].nunique()\n\n# Average length of prompt questions and texts\nprompts_train_df['question_length'] = prompts_train_df['prompt_question'].apply(len)\nprompts_train_df['text_length'] = prompts_train_df['prompt_text'].apply(len)\n\navg_question_length = prompts_train_df['question_length'].mean()\navg_text_length = prompts_train_df['text_length'].mean()\n\nnum_unique_prompts, avg_question_length, avg_text_length\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:58.041040Z","iopub.execute_input":"2023-09-30T10:21:58.041602Z","iopub.status.idle":"2023-09-30T10:21:58.056436Z","shell.execute_reply.started":"2023-09-30T10:21:58.041572Z","shell.execute_reply":"2023-09-30T10:21:58.055264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Setting up the figure and axes\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\n# Plotting the distribution of content scores\nax[0].hist(summaries_train_df['content'], bins=50, color='blue', alpha=0.7)\nax[0].set_title('Distribution of Content Scores')\nax[0].set_xlabel('Content Score')\nax[0].set_ylabel('Number of Summaries')\n\n# Plotting the distribution of wording scores\nax[1].hist(summaries_train_df['wording'], bins=50, color='green', alpha=0.7)\nax[1].set_title('Distribution of Wording Scores')\nax[1].set_xlabel('Wording Score')\nax[1].set_ylabel('Number of Summaries')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:58.057755Z","iopub.execute_input":"2023-09-30T10:21:58.058102Z","iopub.status.idle":"2023-09-30T10:21:58.798928Z","shell.execute_reply.started":"2023-09-30T10:21:58.058076Z","shell.execute_reply":"2023-09-30T10:21:58.797870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating word count for each summary\nsummaries_train_df['word_count'] = summaries_train_df['text'].apply(lambda x: len(str(x).split()))\n\n# Plotting the distribution of word counts\nplt.figure(figsize=(10, 6))\nplt.hist(summaries_train_df['word_count'], bins=50, color='purple', alpha=0.7)\nplt.title('Word Count Distribution in Summaries')\nplt.xlabel('Word Count')\nplt.ylabel('Number of Summaries')\nplt.grid(axis='y')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:58.800175Z","iopub.execute_input":"2023-09-30T10:21:58.800498Z","iopub.status.idle":"2023-09-30T10:21:59.174412Z","shell.execute_reply.started":"2023-09-30T10:21:58.800472Z","shell.execute_reply":"2023-09-30T10:21:59.172982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the number of summaries per prompt\nsummaries_per_prompt = summaries_train_df['prompt_id'].value_counts()\n\n# Plotting the number of summaries per prompt\nplt.figure(figsize=(10, 6))\nsummaries_per_prompt.plot(kind='bar', color='orange', alpha=0.7)\nplt.title('Number of Summaries per Prompt')\nplt.xlabel('Prompt ID')\nplt.ylabel('Number of Summaries')\nplt.grid(axis='y')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:59.175899Z","iopub.execute_input":"2023-09-30T10:21:59.176209Z","iopub.status.idle":"2023-09-30T10:21:59.481464Z","shell.execute_reply.started":"2023-09-30T10:21:59.176184Z","shell.execute_reply":"2023-09-30T10:21:59.480350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating word count for each prompt text\nprompts_train_df['prompt_word_count'] = prompts_train_df['prompt_text'].apply(lambda x: len(str(x).split()))\n\n# Plotting the distribution of word counts for prompts\nplt.figure(figsize=(10, 6))\nplt.bar(prompts_train_df['prompt_id'], prompts_train_df['prompt_word_count'], color='teal', alpha=0.7)\nplt.title('Word Count Distribution in Prompt Texts')\nplt.xlabel('Prompt ID')\nplt.ylabel('Word Count')\nplt.grid(axis='y')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:59.482773Z","iopub.execute_input":"2023-09-30T10:21:59.483084Z","iopub.status.idle":"2023-09-30T10:21:59.723220Z","shell.execute_reply.started":"2023-09-30T10:21:59.483059Z","shell.execute_reply":"2023-09-30T10:21:59.722533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nCertainly! Here's a summarized overview of the Exploratory Data Analysis (EDA) we conducted:\n\nDistribution of Content and Wording Scores:\n\nBoth distributions are roughly normal and centered around a score of approximately 2.5.\nThe dataset contains summaries of diverse quality, as indicated by the range of both low and high scores.\nWord Count Distribution in Summaries:\n\nMost summaries are concise, with word counts primarily ranging from 0 to 100.\nA significant number of summaries have around 20-30 words, while longer summaries (beyond 100 words) are less common.\nNumber of Summaries per Prompt:\n\nThe dataset is balanced in terms of prompt representation, with each prompt having a comparable number of associated summaries.\nWord Count Distribution in Prompt Texts:\n\nPrompts vary in length, with word counts ranging from approximately 100 to over 300 words. This indicates variability in the complexity and length of the prompts provided to students.\nIn summary, the dataset is well-balanced in terms of prompt representation and contains summaries of varied quality and length. The prompts themselves also vary in complexity and length. This diversity is beneficial for building a robust model to predict content and wording scores for student summaries.\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Based on the dataset overview and our EDA, here are some suggested data cleaning steps:\n\n1. **Handling Missing Values**:\n    - Check for missing values in the dataset.\n    - Depending on the number and nature of missing values, decide whether to impute them, drop them, or replace them with placeholder values.\n\n2. **Text Cleaning**:\n    - **Lowercasing**: Convert all the text to lowercase to maintain consistency.\n    - **Punctuation Removal**: Remove punctuation marks, as they might not add significant value for our modeling purposes.\n    - **Stopwords Removal**: Eliminate common words that don't carry significant meaning, like \"and\", \"the\", \"is\", etc. (though this step might be optional based on the model we're using).\n    - **Tokenization**: Break down the text into individual words or tokens.\n    - **Lemmatization/Stemming**: Convert words to their base or root form. For example, \"running\" -> \"run\". This can help in reducing the dataset's dimensionality.\n\n3. **Outliers Handling**:\n    - Based on the distribution of content and wording scores, identify if there are any extreme outliers.\n    - Decide on a strategy to handle these outliers – whether to cap them, transform them, or remove them entirely.\n\n4. **Duplicate Removal**:\n    - Ensure there aren't any duplicate entries in the dataset, especially in the summaries. If found, they should be removed to prevent over-representation.\n\n5. **Standardizing Scores**:\n    - If the scores (content and wording) have vastly different scales, consider standardizing or normalizing them.\n\n6. **Encoding Categorical Data**:\n    - If there are any categorical variables that need to be included in the model, consider encoding them (e.g., using one-hot encoding or label encoding).\n\n7. **Handling Imbalanced Data**:\n    - If the dataset is imbalanced with respect to some categories (e.g., a specific prompt having significantly fewer summaries), consider strategies to balance it, like oversampling, undersampling, or using synthetic data generation techniques.\n\n8. **Text Length Consistency**:\n    - Since the word count of summaries varies, consider setting a consistent length for model input (e.g., using padding for shorter texts or truncating longer ones).\n\n9. **Spell Check and Correction**:\n    - Since the summaries are written by students, there may be spelling mistakes. Depending on the objective, consider running a spell-check and correction.\n\nOnce the data is cleaned and preprocessed, it will be in a more suitable format for modeling and further analysis. Remember, the choice of specific cleaning steps can also depend on the model being used. For instance, models like BERT can handle stopwords and punctuation efficiently, so removing them might not be necessary.","metadata":{}},{"cell_type":"markdown","source":"2. Text Cleaning\nFor this step, we'll focus on the following tasks:\n\nConvert all text to lowercase for consistency.\nRemove punctuation.\nTokenize the text (split it into individual words).\nApply lemmatization to reduce words to their base form.\nLet's start by cleaning the text column in the summaries_train_df dataframe.","metadata":{}},{"cell_type":"code","source":"'''import nltk\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet'''\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:59.726191Z","iopub.execute_input":"2023-09-30T10:21:59.727078Z","iopub.status.idle":"2023-09-30T10:21:59.734565Z","shell.execute_reply.started":"2023-09-30T10:21:59.727046Z","shell.execute_reply":"2023-09-30T10:21:59.733298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport string\nimport spacy\n\n# Load the spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load the stopwords from the uploaded file\nwith open(\"/kaggle/input/nltk-english-stopwords/nltk_eng_stopwords.csv\", \"r\") as f:\n    stop_words = set(f.read().splitlines())\n\n# Initialize lemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = ''.join([char for char in text if char not in string.punctuation])\n    doc = nlp(text)\n    tokens = [word.lemma_ for word in doc if word not in stop_words]\n    return ' '.join(tokens)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:21:59.735976Z","iopub.execute_input":"2023-09-30T10:21:59.736407Z","iopub.status.idle":"2023-09-30T10:22:16.891099Z","shell.execute_reply.started":"2023-09-30T10:21:59.736367Z","shell.execute_reply":"2023-09-30T10:22:16.890126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Applying the cleaning function to the summaries\nsummaries_train_df['cleaned_text'] = summaries_train_df['text'].apply(clean_text)\nprompts_train_df['cleaned_prompt_question']=prompts_train_df['prompt_question'].apply(clean_text)\nprompts_train_df['cleaned_prompt_text']=prompts_train_df['prompt_text'].apply(clean_text)\nsummaries_test_df['cleaned_text'] = summaries_test_df['text'].apply(clean_text)\nprompts_test_df['cleaned_prompt_question']=prompts_test_df['prompt_question'].apply(clean_text)\nprompts_test_df['cleaned_prompt_text']=prompts_test_df['prompt_text'].apply(clean_text)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:22:16.892962Z","iopub.execute_input":"2023-09-30T10:22:16.893798Z","iopub.status.idle":"2023-09-30T10:24:17.351294Z","shell.execute_reply.started":"2023-09-30T10:22:16.893762Z","shell.execute_reply":"2023-09-30T10:24:17.350052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that the values don't show extreme deviations, we may not have significant outliers. ","metadata":{}},{"cell_type":"markdown","source":"No dupicate record found. Now I will be ignoring other data cleaning steps except spell check.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineeering","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:24:17.352713Z","iopub.execute_input":"2023-09-30T10:24:17.353031Z","iopub.status.idle":"2023-09-30T10:24:18.346278Z","shell.execute_reply.started":"2023-09-30T10:24:17.353004Z","shell.execute_reply":"2023-09-30T10:24:18.345307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install transformers\n\nfrom transformers import BertTokenizer\n\ndef truncate_string(input_string, max_length=510, model_name='/kaggle/input/huggingface-bert/bert-base-uncased/'):\n    \"\"\"\n    Truncate the input string to fit within the model's maximum allowable tokens.\n\n    Parameters:\n    - input_string (str): The input string to be truncated.\n    - max_length (int): Maximum number of tokens allowed. Default is 512.\n    - model_name (str): The name of the pretrained model for the tokenizer. Default is 'bert-base-uncased'.\n\n    Returns:\n    - truncated_string (str): The truncated string.\n    \"\"\"\n    \n    # Initialize the tokenizer\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n\n    # Tokenize the string\n    tokens = tokenizer.tokenize(input_string)\n\n    # Check the length and truncate if necessary\n    if len(tokens) > max_length:\n        tokens = tokens[:max_length]\n\n    # Convert the truncated token sequence back to a string\n    truncated_string = tokenizer.convert_tokens_to_string(tokens)\n\n    return truncated_string\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:24:18.347655Z","iopub.execute_input":"2023-09-30T10:24:18.348050Z","iopub.status.idle":"2023-09-30T10:24:18.355068Z","shell.execute_reply.started":"2023-09-30T10:24:18.348011Z","shell.execute_reply":"2023-09-30T10:24:18.353695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nBERT_MODEL_DIR = \"/kaggle/input/huggingface-bert/bert-base-uncased/\"\n\n# Load the tokenizer and model\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_DIR)\nmodel = BertModel.from_pretrained(BERT_MODEL_DIR)\n\n\ndef get_distilbert_embedding(text):\n    text=truncate_string(text)\n    # Tokenize the input text and obtain the output tensors\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():\n        # Get the hidden states of the model\n        outputs = model(**inputs)\n    # Use the mean of the last hidden state as the embedding\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n    return embeddings\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:24:18.356969Z","iopub.execute_input":"2023-09-30T10:24:18.357421Z","iopub.status.idle":"2023-09-30T10:24:24.447346Z","shell.execute_reply.started":"2023-09-30T10:24:18.357382Z","shell.execute_reply":"2023-09-30T10:24:24.446326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:24:24.448783Z","iopub.execute_input":"2023-09-30T10:24:24.449067Z","iopub.status.idle":"2023-09-30T10:24:24.452560Z","shell.execute_reply.started":"2023-09-30T10:24:24.449042Z","shell.execute_reply":"2023-09-30T10:24:24.451511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train_df['summary_embeddings'] = summaries_train_df['cleaned_text'] .apply(get_distilbert_embedding)\nsummaries_test_df['summary_embeddings'] = summaries_test_df['cleaned_text'] .apply(get_distilbert_embedding)\n\n\nprompts_train_df['prompt']=prompts_train_df['cleaned_prompt_question']+prompts_train_df['cleaned_prompt_text']\nprompts_train_df['prompt_embeddings'] = prompts_train_df['prompt'] .apply(get_distilbert_embedding)\n#Lets merge summary and prompt dataframes\nsummaries_train_df=pd.merge(summaries_train_df,prompts_train_df,how=\"inner\", on=\"prompt_id\")\n\n\nprompts_test_df['prompt']=prompts_test_df['cleaned_prompt_question']+prompts_test_df['cleaned_prompt_text']\nprompts_test_df['prompt_embeddings'] = prompts_test_df['prompt'] .apply(get_distilbert_embedding)\n#Lets merge summary and prompt dataframes\nsummaries_test_df=pd.merge(summaries_test_df,prompts_test_df,how=\"inner\", on=\"prompt_id\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:24:24.453696Z","iopub.execute_input":"2023-09-30T10:24:24.454027Z","iopub.status.idle":"2023-09-30T10:49:03.596115Z","shell.execute_reply.started":"2023-09-30T10:24:24.454002Z","shell.execute_reply":"2023-09-30T10:49:03.594905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the cosine similarity between two embeddings\ndef compute_cosine_similarity(embedding1, embedding2):\n    # Reshape the embeddings to 2D (samples, features) for cosine_similarity function\n    embedding1 = np.reshape(embedding1, (1, -1))\n    embedding2 = np.reshape(embedding2, (1, -1))\n    return cosine_similarity(embedding1, embedding2)[0][0]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:03.597517Z","iopub.execute_input":"2023-09-30T10:49:03.597816Z","iopub.status.idle":"2023-09-30T10:49:03.604107Z","shell.execute_reply.started":"2023-09-30T10:49:03.597789Z","shell.execute_reply":"2023-09-30T10:49:03.602946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train_df['summary_size'] = summaries_train_df['cleaned_text'].apply(lambda x: len(str(x).split()))\nsummaries_train_df['prompt_size'] = summaries_train_df['prompt'].apply(lambda x: len(str(x).split()))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:03.605609Z","iopub.execute_input":"2023-09-30T10:49:03.606022Z","iopub.status.idle":"2023-09-30T10:49:03.950586Z","shell.execute_reply.started":"2023-09-30T10:49:03.605989Z","shell.execute_reply":"2023-09-30T10:49:03.949519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_test_df['summary_size'] = summaries_test_df['cleaned_text'].apply(lambda x: len(str(x).split()))\nsummaries_test_df['prompt_size'] = summaries_test_df['prompt'].apply(lambda x: len(str(x).split()))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:03.951597Z","iopub.execute_input":"2023-09-30T10:49:03.951866Z","iopub.status.idle":"2023-09-30T10:49:03.959618Z","shell.execute_reply.started":"2023-09-30T10:49:03.951844Z","shell.execute_reply":"2023-09-30T10:49:03.958335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:03.961089Z","iopub.execute_input":"2023-09-30T10:49:03.961542Z","iopub.status.idle":"2023-09-30T10:49:03.975489Z","shell.execute_reply.started":"2023-09-30T10:49:03.961503Z","shell.execute_reply":"2023-09-30T10:49:03.974080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train_df['cosine_similarity'] = summaries_train_df.apply(lambda row: compute_cosine_similarity(row['summary_embeddings'], row['prompt_embeddings']),\n    axis=1\n)\n\n\nsummaries_test_df['cosine_similarity'] = summaries_test_df.apply(lambda row: compute_cosine_similarity(row['summary_embeddings'], row['prompt_embeddings']),\n    axis=1\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:03.976939Z","iopub.execute_input":"2023-09-30T10:49:03.977798Z","iopub.status.idle":"2023-09-30T10:49:06.107541Z","shell.execute_reply.started":"2023-09-30T10:49:03.977753Z","shell.execute_reply":"2023-09-30T10:49:06.106601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\n# Split the train embedding data into a training set (80%) and a validation set (20%)\ntrain_data, validation_data = train_test_split(summaries_train_df, test_size=0.2, random_state=42)\n\ntrain_data.shape, validation_data.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.108723Z","iopub.execute_input":"2023-09-30T10:49:06.109049Z","iopub.status.idle":"2023-09-30T10:49:06.123399Z","shell.execute_reply.started":"2023-09-30T10:49:06.109024Z","shell.execute_reply":"2023-09-30T10:49:06.122561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=train_data.reset_index()\nvalidation_data=validation_data.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.124596Z","iopub.execute_input":"2023-09-30T10:49:06.124894Z","iopub.status.idle":"2023-09-30T10:49:06.136577Z","shell.execute_reply.started":"2023-09-30T10:49:06.124868Z","shell.execute_reply":"2023-09-30T10:49:06.135564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Developement","metadata":{}},{"cell_type":"code","source":"train_data[\"summary_embeddings\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.138068Z","iopub.execute_input":"2023-09-30T10:49:06.138461Z","iopub.status.idle":"2023-09-30T10:49:06.154355Z","shell.execute_reply.started":"2023-09-30T10:49:06.138433Z","shell.execute_reply":"2023-09-30T10:49:06.152969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.stack(train_data[\"summary_embeddings\"].values)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.155612Z","iopub.execute_input":"2023-09-30T10:49:06.156268Z","iopub.status.idle":"2023-09-30T10:49:06.189177Z","shell.execute_reply.started":"2023-09-30T10:49:06.156217Z","shell.execute_reply":"2023-09-30T10:49:06.188116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['prompt_embeddings'][0]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.190734Z","iopub.execute_input":"2023-09-30T10:49:06.191041Z","iopub.status.idle":"2023-09-30T10:49:06.204516Z","shell.execute_reply.started":"2023-09-30T10:49:06.191016Z","shell.execute_reply":"2023-09-30T10:49:06.203321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.squeeze(np.stack(train_data[\"summary_embeddings\"].values))","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.211456Z","iopub.execute_input":"2023-09-30T10:49:06.211767Z","iopub.status.idle":"2023-09-30T10:49:06.234731Z","shell.execute_reply.started":"2023-09-30T10:49:06.211743Z","shell.execute_reply":"2023-09-30T10:49:06.233991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.stack(train_data[\"summary_embeddings\"].values)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.236009Z","iopub.execute_input":"2023-09-30T10:49:06.236335Z","iopub.status.idle":"2023-09-30T10:49:06.263810Z","shell.execute_reply.started":"2023-09-30T10:49:06.236308Z","shell.execute_reply":"2023-09-30T10:49:06.262779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"summary_embeddings\"].values","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.265333Z","iopub.execute_input":"2023-09-30T10:49:06.265644Z","iopub.status.idle":"2023-09-30T10:49:06.315888Z","shell.execute_reply.started":"2023-09-30T10:49:06.265618Z","shell.execute_reply":"2023-09-30T10:49:06.314464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.stack(train_data[\"prompt_size\"].values)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.317016Z","iopub.execute_input":"2023-09-30T10:49:06.317431Z","iopub.status.idle":"2023-09-30T10:49:06.344674Z","shell.execute_reply.started":"2023-09-30T10:49:06.317401Z","shell.execute_reply":"2023-09-30T10:49:06.343608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = [\n    np.stack(train_data[\"summary_embeddings\"].values),\n    np.stack(train_data[\"prompt_embeddings\"].values),\n    np.stack(train_data[\"summary_size\"].values),\n    np.stack(train_data[\"prompt_size\"].values),\n    np.stack(train_data[\"cosine_similarity\"].values)\n]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.346086Z","iopub.execute_input":"2023-09-30T10:49:06.346693Z","iopub.status.idle":"2023-09-30T10:49:06.413752Z","shell.execute_reply.started":"2023-09-30T10:49:06.346662Z","shell.execute_reply":"2023-09-30T10:49:06.412660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data[\"content\"].values)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.415875Z","iopub.execute_input":"2023-09-30T10:49:06.416209Z","iopub.status.idle":"2023-09-30T10:49:06.422817Z","shell.execute_reply.started":"2023-09-30T10:49:06.416182Z","shell.execute_reply":"2023-09-30T10:49:06.421595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n\n# Compute the cosine similarity between two embeddings\ndef compute_cosine_similarity(embedding1, embedding2):\n    # Reshape the embeddings to 2D (samples, features) for cosine_similarity function\n    embedding1 = np.reshape(embedding1, (1, -1))\n    embedding2 = np.reshape(embedding2, (1, -1))\n    return cosine_similarity(embedding1, embedding2)[0][0]\n\n# Modified Model definition\ndef build_model(embedding_size):\n    # Input layers for summary and prompt embeddings\n    summary_input = Input(shape=(embedding_size,), name=\"summary_embedding\")\n    prompt_input = Input(shape=(embedding_size,), name=\"prompt_embedding\")\n    summary_size_input = Input(shape=(1,), name=\"summary_size\")\n    prompt_size_input = Input(shape=(1,), name=\"prompt_size\")\n    cosine_similarity_input = Input(shape=(1,), name=\"cosine_similarity\")\n\n    # Concatenate the embeddings and the new inputs\n    merged_input = Concatenate()([summary_input, prompt_input, summary_size_input, prompt_size_input, cosine_similarity_input])\n\n    x = Dense(512, activation='relu')(merged_input)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    # Output layers for content and wording scores\n    content_output = Dense(1, name=\"content\")(x)\n    wording_output = Dense(1, name=\"wording\")(x)\n\n    # Compile the model\n    model = Model(inputs=[summary_input, prompt_input, summary_size_input, prompt_size_input, cosine_similarity_input], \n                  outputs=[content_output, wording_output])\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n    return model\nembedding_size = train_data[\"summary_embeddings\"].iloc[0].shape[0]\n\nmodel = build_model(embedding_size)\n\n# Prepare Training Data\nX_train = [\n    np.stack(train_data[\"summary_embeddings\"].values),\n    np.stack(train_data[\"prompt_embeddings\"].values),\n    np.stack(train_data[\"summary_size\"].values),\n    np.stack(train_data[\"prompt_size\"].values),\n    np.stack(train_data[\"cosine_similarity\"].values)\n]\nY_train = [train_data[\"content\"].values, train_data[\"wording\"].values]\n\n# Prepare Validation Data\nX_val = [\n    np.stack(validation_data[\"summary_embeddings\"].values),\n    np.stack(validation_data[\"prompt_embeddings\"].values),\n    np.stack(validation_data[\"summary_size\"].values),\n   np.stack(validation_data[\"prompt_size\"].values),\n   np.stack(validation_data[\"cosine_similarity\"].values)\n]\nY_val = [validation_data[\"content\"].values, validation_data[\"wording\"].values]\n\n# Train the model\nhistory = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50, batch_size=32, verbose=1)'''\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.424419Z","iopub.execute_input":"2023-09-30T10:49:06.425433Z","iopub.status.idle":"2023-09-30T10:49:06.439960Z","shell.execute_reply.started":"2023-09-30T10:49:06.425390Z","shell.execute_reply":"2023-09-30T10:49:06.438709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import BatchNormalization\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.441400Z","iopub.execute_input":"2023-09-30T10:49:06.442522Z","iopub.status.idle":"2023-09-30T10:49:06.459544Z","shell.execute_reply.started":"2023-09-30T10:49:06.442478Z","shell.execute_reply":"2023-09-30T10:49:06.458002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Activation\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.460873Z","iopub.execute_input":"2023-09-30T10:49:06.462079Z","iopub.status.idle":"2023-09-30T10:49:06.471912Z","shell.execute_reply.started":"2023-09-30T10:49:06.462032Z","shell.execute_reply":"2023-09-30T10:49:06.471122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = [\n    np.stack(train_data[\"summary_embeddings\"].values),\n    np.stack(train_data[\"prompt_embeddings\"].values),\n    train_data[\"summary_size\"].values,\n    train_data[\"prompt_size\"].values,\n    train_data[\"cosine_similarity\"].values\n]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.473203Z","iopub.execute_input":"2023-09-30T10:49:06.474440Z","iopub.status.idle":"2023-09-30T10:49:06.525160Z","shell.execute_reply.started":"2023-09-30T10:49:06.474397Z","shell.execute_reply":"2023-09-30T10:49:06.523973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data[\"summary_embeddings\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.526478Z","iopub.execute_input":"2023-09-30T10:49:06.526928Z","iopub.status.idle":"2023-09-30T10:49:06.533747Z","shell.execute_reply.started":"2023-09-30T10:49:06.526842Z","shell.execute_reply":"2023-09-30T10:49:06.532537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"summary_embeddings\"].values","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.535587Z","iopub.execute_input":"2023-09-30T10:49:06.535884Z","iopub.status.idle":"2023-09-30T10:49:06.582806Z","shell.execute_reply.started":"2023-09-30T10:49:06.535853Z","shell.execute_reply":"2023-09-30T10:49:06.581972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"summary_size\"].values","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.584340Z","iopub.execute_input":"2023-09-30T10:49:06.585326Z","iopub.status.idle":"2023-09-30T10:49:06.591456Z","shell.execute_reply.started":"2023-09-30T10:49:06.585295Z","shell.execute_reply":"2023-09-30T10:49:06.590307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"summary_embeddings\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.592945Z","iopub.execute_input":"2023-09-30T10:49:06.593274Z","iopub.status.idle":"2023-09-30T10:49:06.611860Z","shell.execute_reply.started":"2023-09-30T10:49:06.593223Z","shell.execute_reply":"2023-09-30T10:49:06.610801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Prepare data for training\nX_train = [\n    np.stack(train_data[\"summary_embeddings\"].values),\n    np.stack(train_data[\"prompt_embeddings\"].values),\n    train_data[\"summary_size\"].values,\n    train_data[\"prompt_size\"].values,\n    train_data[\"cosine_similarity\"].values\n]\n\nY_train = [train_data[\"content\"].values, train_data[\"wording\"].values]\n\nX_val = [\n    np.stack(validation_data[\"summary_embeddings\"].values),\n    np.stack(validation_data[\"prompt_embeddings\"].values),\n    validation_data[\"summary_size\"].values,\n    validation_data[\"prompt_size\"].values,\n    validation_data[\"cosine_similarity\"].values\n]\n\nY_val = [validation_data[\"content\"].values, validation_data[\"wording\"].values]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.613620Z","iopub.execute_input":"2023-09-30T10:49:06.614287Z","iopub.status.idle":"2023-09-30T10:49:06.659752Z","shell.execute_reply.started":"2023-09-30T10:49:06.614254Z","shell.execute_reply":"2023-09-30T10:49:06.658984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### HyperParameterTuning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.660699Z","iopub.execute_input":"2023-09-30T10:49:06.661296Z","iopub.status.idle":"2023-09-30T10:49:06.666929Z","shell.execute_reply.started":"2023-09-30T10:49:06.661260Z","shell.execute_reply":"2023-09-30T10:49:06.665812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kerastuner import RandomSearch\n\ndef build_model(hp):\n    # Input layers\n    summary_input = Input(shape=(768,), name=\"summary_embedding\")\n    prompt_input = Input(shape=(768,), name=\"prompt_embedding\")\n    \n    # Process embeddings separately\n    merged_embeddings = Concatenate()([summary_input, prompt_input])\n    \n    # Define the hyperparameter search space for the number of units in the first Dense layer\n    units_1 = hp.Int('units_1', min_value=512, max_value=2560, step=256)\n    \n    x = Dense(units_1, activation='relu')(merged_embeddings)\n    \n    # Dropout hyperparameter\n    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n    embeddings_processed = Dropout(dropout_rate)(x)\n    \n    # Subsequent layers with halved neurons\n    units_2 = units_1 // 2\n    x = Dense(units_2, activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n\t\n\t# For scalar inputs, we'll use BatchNormalization followed by a sigmoid activation to ensure values are in [0, 1]\n    summary_size_input = Input(shape=(1,), name=\"summary_size\")\n    normalized_summary_size = BatchNormalization()(summary_size_input)\n    scaled_summary_size = Activation('sigmoid')(normalized_summary_size)\n\n    prompt_size_input = Input(shape=(1,), name=\"prompt_size\")\n    normalized_prompt_size = BatchNormalization()(prompt_size_input)\n    scaled_prompt_size = Activation('sigmoid')(normalized_prompt_size)\n\n    cosine_similarity_input = Input(shape=(1,), name=\"cosine_similarity\")\n   \n\n    # Concatenate the processed embeddings and the scaled new inputs\n    merged_input = Concatenate()([embeddings_processed, scaled_summary_size, scaled_prompt_size, cosine_similarity_input])\n    \n    # Further processing (considering the dimensionality)\n    \n    units_3 = units_2 // 2\n    x = Dense(units_3, activation='relu')(merged_input)\n    x = Dropout(dropout_rate)(x)\n    \n    units_4 = units_3 // 2\n    x = Dense(units_4, activation='relu')(x)\n    x = Dropout(dropout_rate)(x)\n  \n    # Output layers for content and wording scores\n    content_output = Dense(1, name=\"content\")(x)\n    wording_output = Dense(1, name=\"wording\")(x)\n    \n    # Compile the model\n    model = Model(inputs=[summary_input, prompt_input, summary_size_input, prompt_size_input, cosine_similarity_input], \n                  outputs=[content_output, wording_output])\n\n    # Optimizer hyperparameter\n    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n    \n    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n    return model\n\n# Create a tuner\ntuner = RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=20,  # Number of hyperparameter combinations to try\n    directory='keras_tuner_directory',\n    project_name='keras_tuner_demo'\n)\n\n# Display search space summary\ntuner.search_space_summary()\n\n# Start hyperparameter search\ntuner.search(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50, batch_size=32)\n\n# Display results\ntuner.results_summary()\n\n# Retrieve the best model\nbest_model = tuner.get_best_models(num_models=1)[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T10:49:06.668701Z","iopub.execute_input":"2023-09-30T10:49:06.669292Z","iopub.status.idle":"2023-09-30T11:56:46.488619Z","shell.execute_reply.started":"2023-09-30T10:49:06.669233Z","shell.execute_reply":"2023-09-30T11:56:46.487418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get the best hyperparamters","metadata":{}},{"cell_type":"code","source":"# Get the best hyperparameters\nbest_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n\n# Display the best hyperparameters\nprint(\"Best Hyperparameters:\")\nprint(best_hyperparameters.values)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T11:56:46.498043Z","iopub.execute_input":"2023-09-30T11:56:46.498592Z","iopub.status.idle":"2023-09-30T11:56:46.511460Z","shell.execute_reply.started":"2023-09-30T11:56:46.498562Z","shell.execute_reply":"2023-09-30T11:56:46.510615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lv_units_1=best_hyperparameters['units_1']\nlv_dropout_rate=best_hyperparameters['dropout_rate']\nlv_optimizer=best_hyperparameters['optimizer']\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:03:09.276562Z","iopub.execute_input":"2023-09-30T13:03:09.277007Z","iopub.status.idle":"2023-09-30T13:03:09.282968Z","shell.execute_reply.started":"2023-09-30T13:03:09.276975Z","shell.execute_reply":"2023-09-30T13:03:09.281897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kerastuner import RandomSearch\n\ndef model_instance(lv_units_1,lv_dropout_rate,lv_optimizer):\n    # Input layers\n    summary_input = Input(shape=(768,), name=\"summary_embedding\")\n    prompt_input = Input(shape=(768,), name=\"prompt_embedding\")\n    \n    # Process embeddings separately\n    merged_embeddings = Concatenate()([summary_input, prompt_input])\n    \n   \n    x = Dense(lv_units_1, activation='relu')(merged_embeddings)\n    \n   \n    embeddings_processed = Dropout(lv_dropout_rate)(x)\n    \n    # Subsequent layers with halved neurons\n    units_2 = lv_units_1 // 2\n    x = Dense(units_2, activation='relu')(x)\n    x = Dropout(lv_dropout_rate)(x)\n\t\n\t# For scalar inputs, we'll use BatchNormalization followed by a sigmoid activation to ensure values are in [0, 1]\n    summary_size_input = Input(shape=(1,), name=\"summary_size\")\n    normalized_summary_size = BatchNormalization()(summary_size_input)\n    scaled_summary_size = Activation('sigmoid')(normalized_summary_size)\n\n    prompt_size_input = Input(shape=(1,), name=\"prompt_size\")\n    normalized_prompt_size = BatchNormalization()(prompt_size_input)\n    scaled_prompt_size = Activation('sigmoid')(normalized_prompt_size)\n\n    cosine_similarity_input = Input(shape=(1,), name=\"cosine_similarity\")\n   \n\n    # Concatenate the processed embeddings and the scaled new inputs\n    merged_input = Concatenate()([embeddings_processed, scaled_summary_size, scaled_prompt_size, cosine_similarity_input])\n    \n    # Further processing (considering the dimensionality)\n    \n    units_3 = units_2 // 2\n    x = Dense(units_3, activation='relu')(merged_input)\n    x = Dropout(lv_dropout_rate)(x)\n    \n    units_4 = units_3 // 2\n    x = Dense(units_4, activation='relu')(x)\n    x = Dropout(lv_dropout_rate)(x)\n  \n    # Output layers for content and wording scores\n    content_output = Dense(1, name=\"content\")(x)\n    wording_output = Dense(1, name=\"wording\")(x)\n    \n    # Compile the model\n    model = Model(inputs=[summary_input, prompt_input, summary_size_input, prompt_size_input, cosine_similarity_input], \n                  outputs=[content_output, wording_output])\n\n   \n    model.compile(optimizer=lv_optimizer, loss='mse', metrics=['mse'])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:22:57.738277Z","iopub.execute_input":"2023-09-30T13:22:57.738683Z","iopub.status.idle":"2023-09-30T13:22:57.751410Z","shell.execute_reply.started":"2023-09-30T13:22:57.738651Z","shell.execute_reply":"2023-09-30T13:22:57.750493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lv_model_instance = model_instance(lv_units_1,lv_dropout_rate,lv_optimizer)\n\n# Train the model\nhistory = lv_model_instance.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50, batch_size=32, verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:22:59.569628Z","iopub.execute_input":"2023-09-30T13:22:59.570602Z","iopub.status.idle":"2023-09-30T13:24:59.099057Z","shell.execute_reply.started":"2023-09-30T13:22:59.570565Z","shell.execute_reply":"2023-09-30T13:24:59.097897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Neural Network ","metadata":{}},{"cell_type":"code","source":"# Return the final loss and mean absolute error for the validation set\nfinal_mae_content = history.history['val_content_mse'][-1]\nfinal_mae_wording = history.history['val_wording_mse'][-1]\n\nfinal_mae_content, final_mae_wording","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:24:59.101236Z","iopub.execute_input":"2023-09-30T13:24:59.101559Z","iopub.status.idle":"2023-09-30T13:24:59.109635Z","shell.execute_reply.started":"2023-09-30T13:24:59.101531Z","shell.execute_reply":"2023-09-30T13:24:59.108409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming the summaries_test_df dataframe has a column named 'student_id'\nstudent_ids = summaries_test_df[\"student_id\"].values\n\n\nX_test = [\n    np.stack(summaries_test_df[\"summary_embeddings\"].values),\n    np.stack(summaries_test_df[\"prompt_embeddings\"].values),\n    np.stack(summaries_test_df[\"summary_size\"].values),\n    np.stack(summaries_test_df[\"prompt_size\"].values),\n    np.stack(summaries_test_df[\"cosine_similarity\"].values)\n]\ncontent_preds, wording_preds = lv_model_instance.predict(X_test)\n\n# Format and save predictions\nsubmission_df = pd.DataFrame({\n    'student_id': student_ids,\n    'content': content_preds.flatten(),\n    'wording': wording_preds.flatten()\n})\nsubmission_df.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:43:18.529686Z","iopub.execute_input":"2023-09-30T13:43:18.530323Z","iopub.status.idle":"2023-09-30T13:43:18.987158Z","shell.execute_reply.started":"2023-09-30T13:43:18.530277Z","shell.execute_reply":"2023-09-30T13:43:18.986197Z"},"trusted":true},"execution_count":null,"outputs":[]}]}